{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdGj+Hgox10bW5VylRp49h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendanpshea/logic-prolog/blob/main/The_ProbabilityOfMurder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions (Python)\n",
        "THis chapter uses Python code to calcuate probabilities."
      ],
      "metadata": {
        "id": "IloOvZv93gKo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ECoZc0tFkjnp"
      },
      "outputs": [],
      "source": [
        "def validate_probabilities(*probs):\n",
        "  \"\"\"Validates that each probability value is between 0 and 1.\"\"\"\n",
        "  if any(not 0 <= p <= 1 for p in probs):\n",
        "    raise ValueError(\"Probabilities must be between 0 and 1.\")\n",
        "\n",
        "def complement_rule(pr_e):\n",
        "    \"\"\"Calculates the probability that event E doesn't occur.\"\"\"\n",
        "    validate_probabilities(pr_e)\n",
        "    complement = 1 - pr_e\n",
        "    return f\"P(¬E) = 1 - P(E) = 1 - {pr_e} = {complement}\"\n",
        "\n",
        "def conditional_probability(pr_a, pr_b, pr_a_and_b):\n",
        "    \"\"\"Calcutates the probability of A, given that B is true\"\"\"\n",
        "    validate_probabilities(pr_a, pr_b, pr_a_and_b)\n",
        "    if pr_b == 0:\n",
        "        raise ValueError(\"P(B) cannot be 0.\")\n",
        "    pr_a_given_b = pr_a_and_b / pr_b\n",
        "    return f\"P(A|B) = P(A ∩ B) / P(B) = {pr_a_and_b} / {pr_b} = {pr_a_given_b}\"\n",
        "\n",
        "def complete_addition(pr_e1, pr_e2, pr_e1_and_e2):\n",
        "    \"\"\"Calculates the probability either E1 or E2 occur.\"\"\"\n",
        "    validate_probabilities(pr_e1, pr_e2, pr_e1_and_e2)\n",
        "    result = pr_e1 + pr_e2 - pr_e1_and_e2\n",
        "    return f\"P(E1 ∪ E2) = P(E1) + P(E2) - P(E1 ∩ E2) = {pr_e1} + {pr_e2} - {pr_e1_and_e2} = {result}\"\n",
        "\n",
        "def simple_multiplication(pr_e1, pr_e2):\n",
        "    \"\"\"Calculates the probability both E1 and E2 occur (assumes independence).\"\"\"\n",
        "    validate_probabilities(pr_e1, pr_e2)\n",
        "    result = pr_e1 * pr_e2\n",
        "    return f\"P(E1 ∩ E2) = P(E1) * P(E2) = {pr_e1} * {pr_e2} = {result}\"\n",
        "\n",
        "def complete_multiplication(pr_e1, pr_e2, pr_e1_given_e2):\n",
        "    \"\"\"Calculates the probability bott E1 and E2 occur (may not be indpendent).\"\"\"\n",
        "    validate_probabilities(pr_e1, pr_e2, pr_e1_given_e2)\n",
        "    result = pr_e1_given_e2 * pr_e2\n",
        "    return f\"P(E1 ∩ E2) = P(E1|E2) * P(E2) = {pr_e1_given_e2} * {pr_e2} = {result}\"\n",
        "\n",
        "def total_probability(pr_e_given_h1, pr_h1, pr_e_given_h2, pr_h2):\n",
        "    \"\"\"\n",
        "    Calculates the total probability of an event E given two exclusive hypotheses H1 and H2.\n",
        "    \"\"\"\n",
        "    validate_probabilities(pr_e_given_h1, pr_h1, pr_e_given_h2, pr_h2)\n",
        "    pr_e = pr_e_given_h1 * pr_h1 + pr_e_given_h2 * pr_h2\n",
        "    return (f\"P(E) = P(E|H1) * P(H1) + P(E|H2) * P(H2)\\n\"\n",
        "            f\"     = {pr_e_given_h1} * {pr_h1} + {pr_e_given_h2} * {pr_h2}\\n\"\n",
        "            f\"     = {pr_e}\")\n",
        "def bayes_theorem(p_h, pr_e_given_h, pr_e_given_not_h):\n",
        "    \"\"\"Calculates probability of a hypotheses being true, given some evidence.\"\"\"\n",
        "    validate_probabilities(p_h, pr_e_given_h, pr_e_given_not_h)\n",
        "    p_not_h = 1 - p_h\n",
        "    pr_e = pr_e_given_h * p_h + pr_e_given_not_h * p_not_h\n",
        "    p_h_given_e = (pr_e_given_h * p_h) / pr_e\n",
        "    print(f\"P(H|E) = (P(E|H) * P(H)) / [P(E|H) * P(H) + P(E|not H) * P(not H)]\")\n",
        "    print(f\"       = ({pr_e_given_h} * {p_h}) / ({pr_e_given_h} * {p_h} + {pr_e_given_not_h} * {p_not_h})\")\n",
        "    print(f\"       = {round(p_h_given_e,2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Probability of...Murder\n",
        "In this chapter, we'll exploring the role that **probability** plays in arugments and reasoning. Probability is, at its most basic, the measure of how likely something is to occur, a concept that is as pivotal in statistics as it is in the realm of detective work. In order to explore probability, we'll be enlisting the help of some famous (fictional) detectives, from the sharp Sherlock Holmes to the perceptive Nancy Drew, to see how we can use probability to weave together bits of reality into a coherent picture of truth.\n",
        "\n",
        "To get started, let's imagine that detective Adrian Monk, known keen observational skills, stands in the midst of a crime scene. He recalls that knowledge that 80% of similar crimes were committed using a particular method, he observes the same pattern at his current crime scene. This statistical insight leads Monk to a calculated conclusion: there's a high probability that these crimes are linked. Here, probability invovles concrete numbers and known frequencies. We'll later call this \"frequency-type probability.\"\n",
        "\n",
        "In contrast, let's consider the investigative approach of Velma from Scooby-Doo. In a mysterious mansion, she uncovers a concealed passage, subtly shifting the odds in favor of her hypothesis: the supposed ghost is merely a person exploiting these hidden corridors. Each clue Velma encounters – be it an unusual footprint or a specific thread of fabric – doesn’t just add to her evidence pile; it incrementally adjusts the likelihood of her theories being accurate. Her method is less about direct calculations and more about intuitively assessing how each piece of evidence modifies her hypotheses. We'll later cll this \"belief-type probability.\"\n",
        "\n",
        "For both Monk and Velma, probability is an ever-present guide. It helps them navigate through a landscape of uncertainty and ambiguity, turning each clue into a stepping stone towards the truth. This chapter will take us on a journey through the nuanced streets of probability and logic, where every clue carries its weight in the grand scheme of things.\n"
      ],
      "metadata": {
        "id": "p8-LTDh6xRmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Probability, Part 1: The Kolmogorov Axioms\n",
        "Tucked away in the back alleys of mathematical theory, like a cryptic clue in a detective's notebook, are the **Kolmogorov Axioms**. These axioms are the backbone of probability theory, named after the Russian mathematician Andrey Kolmogorov, who laid down the fundamental principles of probability in a rigorous mathematical way. But fear not, for these axioms are not as daunting as they might seem and can be understood without delving deep into complex mathematics.\n",
        "\n",
        "To begin, we use notation to simplify our discussion. When we talk about the probability of an event, we use the notation *Pr(Event)*. Think of it like saying, \"What are the odds of this happening?\" For example, Sherlock Holmes might calculate the probability of a suspect being at the crime scene, which we could write as *Pr(Suspect at Crime Scene)*. Similarly, when we want to talk about the probability of a hypothesis given a specific event, we use the notation *Pr(Hypothesis|Event)*. It's like asking, \"Given that this clue or event has occurred, what's the probability that my hypothesis is true?\" This is something a detective like Nancy Drew might ponder when she finds a new clue and reassesses her theories.\n",
        "\n",
        "The Kolmogorov Axioms *define* the mathematical notion of probability. They are as follows:\n",
        "\n",
        "1. **Non-negativity.** Every event E has a probability that is a non-negative number:\n",
        "  - $Pr(E) ≥ 0$.\n",
        "2. **Certainty:**  The probability of a certain (or guaranteed) event is 1. For example, the probability of \"an event E either happens or it doesn't happen\" should be 1.\n",
        "  - $Pr(E \\vee \\neg E) = 1$, where E is any event.\n",
        "3. **Additivity.** For any two muually exclusive events (events that cannot both occur at the same time), the probability of either event occurring is the sum of their individual probabilities:\n",
        "  - $Pr(A \\vee B) = Pr(A) + Pr(B)$, for mutually exclusive events A and B.\n",
        "\n",
        "The first axiom of Kolmogorov is that the probability of any event is a non-negative number. This simply means that you can't have a negative chance of something happening. It's either going to happen, or it isn't, or somewhere in between, but it's never less than zero. It's like saying, \"There's no chance that the victim committed the crime,\" which would be a probability of zero, or \"There's a certain chance that the butler did it,\" which might be a probability close to one, but never negative.\n",
        "\n",
        "The second axiom states that the probability of a certain event (one that is guaranteed to happen) is 1. In our detective story, this would be akin to saying, \"The crime definitely happened here,\" which is an absolute certainty and thus has a probability of 1.\n",
        "\n",
        "The third axiom is a bit more complex. It involves the probability of the union of two mutually exclusive events. In simple terms, if you have two events that cannot happen at the same time (like the suspect can't be both in the library and the dining room at the same moment), then the probability that either one happens is the sum of the probabilities of each happening individually. For example, if there's a 30% chance the suspect was in the library and a 40% chance they were in the dining room, and these two events are mutually exclusive, the probability of the suspect being in either location is 70%."
      ],
      "metadata": {
        "id": "gsObJnFzZQYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some Rules for Calculating Probabilities\n",
        "\n",
        "It isn't easy to directly apply the Kolmogorov axioms to calculate probabilities. Luckily, we don't have to! Instead, we can use various derived rules (logicians might call them *theorems*) to make our lives easier. Here are a few that might come in handy.\n",
        "\n",
        "### Complement Rule\n",
        "\n",
        "The complement rule states that the probability of an event not occurring is 1 minus the probability of the event occurring. Mathematically, it's expressed as\n",
        "- Pr(not E) = 1 - Pr(E), where E is any event.\n",
        "\n",
        "Enola and Mycroft Holmes (Sherlock's brother and sister) are investigating a case where they know the probability of a suspect being in London is 0.65. Using the complement rule, they deduce that the probability of the suspect not being in London is 1 - 0.65 = 0.35. This calculation helps the Holmes team strategize their investigation based on the suspect's likely whereabouts."
      ],
      "metadata": {
        "id": "dkhBbNHl0-9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computer code to do this. Try changing the number!\n",
        "complement_rule(pr_e = 0.65)"
      ],
      "metadata": {
        "id": "N0dekaZu379P",
        "outputId": "058b5ae4-937d-43f3-dcea-77daef6ffeec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'P(¬E) = 1 - P(E) = 1 - 0.65 = 0.35'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Probability, Part 2: Two Types of Probability\n",
        "\n",
        "The previous sections tell us how probability works \"mathematically.\" However, the equations we looked at don't tell us what these numbers *mean*. In probability, much like in a detective's investigation, the interpretation of evidence can take different forms. Two fundamental types of probability interpretations are Frequency-Type Probability and Belief-Type Probability. Each type offers a differents lens through which we can understand the meaning behind the numbers in probability.\n",
        "\n",
        "### Frequency-Type Probability\n",
        "\n",
        "**Frequency-Type Probability** is grounded in the long-run frequency of events. It is defined as the limit of the relative frequency of an event occurring after many trials. It is alway relative to a **reference class** describing the results of these trials. In simpler terms, it's about how often something happens over repeated trials or occurrences. It is sometimes called *objective probability*, *chance*, or *physical probability*.\n",
        "\n",
        "-   Ex1: Imagine Sherlock Holmes is investigating a series of burglaries. He finds that in 60 out of 100 past burglary cases in the area (our reference class), the perpetrator left behind a specific clue. Here, the frequency-type probability of finding this clue in a burglary is .6 (or 60%). Holmes might use this probability to gauge the likelihood of encountering this clue in future burglary cases.\n",
        "- Ex2: Veronica Mars has discovered that someone has become gravely ill after taking two aspirin. SHe wonders what the probability that this might happen by chance (for example, because of a drug allergy or manufacturing defect). She looks up the data, and finds out this happens only for only 1 out of 1,000,000 people, which gives the (very low!) probability of 0.00001. She begins to suspect poison.\n",
        "\n",
        "### Belief-Type Probability\n",
        "\n",
        "**Belief-Type Probability**, on the other hand, reflects a degree of belief or confidence that a person has in the occurrence of an event. It is always relative to a person's **total evidence**. It is sometimes called *subjective probability*, *inductive probability* or *logical probability*.\n",
        "\n",
        "-   Ex1: Nancy Drew is investigating a mysterious disappearance. Based on her investigation, she estimates there's a 0.7 (70%) chance that the person disappeared voluntarily. This belief is based on her evidence concerning the case, the person's behavior patterns, and the evidence she has gathered. Her colleague, however, might assess the situation differently based on his perspective and information, assigning a different belief-type probability to the same hypothesis.\n",
        "- Ex2: Lisbeth Salander is analyzing whether a leak within a company was an inside job. She thinks about her total evidence. Given the restricted access to the information, the employees' profiles, and recent unusual network activities, she approximates a 65% probability that the leak was internal. Her estimation is based on her analysis of network security data, employee access levels, and behavior patterns. These factors collectively shape her belief about the likelihood of an internal leak, though it remains an educated guess.\n",
        "\n",
        "### Table: Two Types of Probability\n",
        "In detective work, as in the rest of life, both interpetations of probability are important.\n",
        "\n",
        "| Feature | Frequency-Type Probability | Belief-Type Probability |\n",
        "| --- | --- | --- |\n",
        "| Definition | Defined as the limit of the relative frequency of an event occurring after many trials. | Reflects a degree of belief or confidence in the occurrence of an event, relative to a person's total evidence. |\n",
        "| Reference | Always relative to a specific reference class describing the results of trials or occurrences. | Always relative to a person's total evidence, incorporating both objective data and subjective interpretation. |\n",
        "| Terminology | Also known as objective probability, chance, or physical probability. | Also known as subjective probability, inductive probability, or logical probability. |\n",
        "| Mathematical Formulation | No difference. Probability values range from 0 to 1, where 0 indicates impossibility and 1 indicates certainty. | No difference. Probability values range from 0 to 1, where 0 indicates impossibility and 1 indicates certainty.  |\n",
        "| Nature | Considered more objective, as it is based on empirical data and observed frequencies. | Considered more subjective, as it incorporates personal judgment and interpretation of evidence. |\n",
        "| Application | Useful in situations where data from repeated trials or historical patterns are available. | Useful in decision-making processes where personal judgment and the assessment of all available evidence are key. |"
      ],
      "metadata": {
        "id": "XaLUx1eyu1St"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tAV3uqc-3gGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bayes_theorem(p_h = 0.5,\n",
        "              pr_e_given_h = .9,\n",
        "              pr_e_given_not_h = .2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwXjKJIcljPH",
        "outputId": "79db9d11-1c08-4703-8810-6e3ce9a74696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(H|E) = (P(E|H) * P(H)) / [P(E|H) * P(H) + P(E|not H) * P(not H)]\n",
            "       = (0.9 * 0.5) / (0.9 * 0.5 + 0.2 * 0.5)\n",
            "       = 0.82\n"
          ]
        }
      ]
    }
  ]
}